{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f5a7361-7b51-4167-aaad-726ceb0810a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Layer: Container Status with Structured Streaming\n",
    "\n",
    "**Ingestion Pattern**: Spark Structured Streaming\n",
    "\n",
    "**Features**:\n",
    "- Enable Change Data Feed (CDF) for CDC pattern\n",
    "- Watermarking for late data handling\n",
    "- Deduplication using event time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "316b3d60-5d72-431c-bde7-8948d9fe9fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import logging\n",
    "\n",
    "# Import utilities\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from logging_utils import get_logger\n",
    "\n",
    "logger = get_logger(\"bronze_status_streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e13487-7008-4584-8e85-db40043767f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get parameters\n",
    "dbutils.widgets.text(\"catalog_name\", \"cargo_fleet_dev\", \"Catalog\")\n",
    "dbutils.widgets.text(\"checkpoint_location\", \"\", \"Checkpoint Location\")\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "checkpoint_location = dbutils.widgets.get(\"checkpoint_location\") or \\\n",
    "    f\"/Volumes/{catalog_name}/bronze/checkpoints/status_streaming\"\n",
    "\n",
    "logger.info(f\"Starting Structured Streaming for container status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e0465bc-bada-4360-9440-cea383ec7e23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "status_schema = StructType([\n",
    "    StructField(\"container_id\", StringType(), False),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"checkpoint_type\", StringType(), True),\n",
    "    StructField(\"checkpoint_time\", StringType(), True),\n",
    "    StructField(\"temperature_celsius\", DoubleType(), True),\n",
    "    StructField(\"humidity_percent\", DoubleType(), True),\n",
    "    StructField(\"seal_intact\", BooleanType(), True),\n",
    "    StructField(\"inspected_by\", StringType(), True),\n",
    "    StructField(\"notes\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82b8e17b-c70c-4a3a-9381-166093a82c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_path = f\"/Volumes/{catalog_name}/bronze/status_landing\"\n",
    "\n",
    "try:\n",
    "    df_status_stream = (\n",
    "        spark.readStream\n",
    "        .format(\"json\")\n",
    "        .schema(status_schema)\n",
    "        .option(\"maxFilesPerTrigger\", 50)\n",
    "        .load(source_path)\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Stream configured for source: {source_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to configure stream: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871fd276-971e-48ce-b91f-212703fcb3bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform with Watermarking\n",
    "\n",
    "try:\n",
    "    df_status_bronze = (\n",
    "        df_status_stream\n",
    "        # Convert checkpoint_time to timestamp\n",
    "        .withColumn(\"checkpoint_time\", to_timestamp(col(\"checkpoint_time\")))\n",
    "        \n",
    "        # Add watermark for late data (1 hour threshold)\n",
    "        .withWatermark(\"checkpoint_time\", \"1 hour\")\n",
    "        \n",
    "        # Remove duplicates based on container_id and checkpoint_time\n",
    "        .dropDuplicates([\"container_id\", \"checkpoint_time\"])\n",
    "        \n",
    "        # Add metadata\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_file\", col(\"_metadata.file_path\"))\n",
    "        .withColumn(\"ingestion_date\", current_date())\n",
    "        \n",
    "        # Add data quality checks\n",
    "        .withColumn(\"is_temperature_valid\",\n",
    "            col(\"temperature_celsius\").between(-30, 50)\n",
    "        )\n",
    "        .withColumn(\"is_humidity_valid\",\n",
    "            col(\"humidity_percent\").between(0, 100)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Watermarking and transformations applied\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to apply transformations: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c4b361-2108-431a-8d24-1f6310235510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Write Stream with CDF Enabled\n",
    "bronze_table = f\"{catalog_name}.bronze.container_status_raw\"\n",
    "\n",
    "try:\n",
    "    # First, ensure the table exists with CDF enabled\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {bronze_table} (\n",
    "            container_id STRING,\n",
    "            status STRING,\n",
    "            location STRING,\n",
    "            checkpoint_type STRING,\n",
    "            checkpoint_time TIMESTAMP,\n",
    "            temperature_celsius DOUBLE,\n",
    "            humidity_percent DOUBLE,\n",
    "            seal_intact BOOLEAN,\n",
    "            inspected_by STRING,\n",
    "            notes STRING,\n",
    "            ingestion_timestamp TIMESTAMP,\n",
    "            source_file STRING,\n",
    "            ingestion_date DATE,\n",
    "            is_temperature_valid BOOLEAN,\n",
    "            is_humidity_valid BOOLEAN\n",
    "        )\n",
    "        USING DELTA\n",
    "        PARTITIONED BY (ingestion_date)\n",
    "        TBLPROPERTIES (\n",
    "            delta.enableChangeDataFeed = true,\n",
    "            delta.autoOptimize.optimizeWrite = true,\n",
    "            delta.autoOptimize.autoCompact = true\n",
    "        )\n",
    "        COMMENT 'Container status changes with CDF enabled for CDC pattern'\n",
    "    \"\"\")\n",
    "    \n",
    "    logger.info(f\"✓ Target table created with CDF enabled: {bronze_table}\")\n",
    "    \n",
    "    # Write stream\n",
    "    query = (\n",
    "        df_status_bronze.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", checkpoint_location)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .partitionBy(\"ingestion_date\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(bronze_table)\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"✓ Streaming query started with watermarking\")\n",
    "    logger.info(f\"✓ Late data threshold: 1 hour\")\n",
    "    logger.info(f\"✓ Query ID: {query.id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to start streaming query: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff3a232-7a86-45ec-ae82-7a0a1bf8b5c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Keep the stream running\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest_status_streaming.py",
   "widgets": {
    "catalog_name": {
     "currentValue": "cargo_fleet_dev",
     "nuid": "4bda76ae-deba-4b75-a66a-40f73d7f3573",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "cargo_fleet_dev",
      "label": "Catalog",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "cargo_fleet_dev",
      "label": "Catalog",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "checkpoint_location": {
     "currentValue": "",
     "nuid": "dc514f0c-cb4e-4e9d-88b6-166c5ae195d4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Checkpoint Location",
      "name": "checkpoint_location",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Checkpoint Location",
      "name": "checkpoint_location",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
